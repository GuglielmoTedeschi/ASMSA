{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19958a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install keras_tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37862bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "from scipy.stats import gaussian_kde\n",
    "import matplotlib.pyplot as plt\n",
    "from src import asmsa\n",
    "from src.gan import GAN\n",
    "from src.visualizer import GAN_visualizer\n",
    "import mdtraj as md\n",
    "import nglview as nv\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import Input, Dense, Reshape, Flatten\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.callbacks import CSVLogger, EarlyStopping\n",
    "from keras.models import Sequential, Model\n",
    "from keras.losses import BinaryCrossentropy, MeanSquaredError\n",
    "from keras import backend as kb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa58e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create session due to insufficient vram error\n",
    "\n",
    "config = tf.compat.v1.ConfigProto(gpu_options = \n",
    "                         tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.8)\n",
    "# device_count = {'GPU': 1}\n",
    ")\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)\n",
    "tf.compat.v1.keras.backend.set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50646ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input files\n",
    "%cd ~\n",
    "\n",
    "# input conformation\n",
    "#conf = \"alaninedipeptide_H.pdb\"\n",
    "conf = \"trpcage_correct.pdb\"\n",
    "\n",
    "# input trajectory\n",
    "# atom numbering must be consistent with {conf}\n",
    "\n",
    "#traj = \"alaninedipeptide_reduced.xtc\"\n",
    "traj = \"trpcage_red.xtc\"\n",
    "\n",
    "# input topology\n",
    "# expected to be produced with \n",
    "#    gmx pdb2gmx -f {conf} -p {topol} -n {index} \n",
    "\n",
    "# Gromacs changes atom numbering, the index file must be generated and used as well\n",
    "\n",
    "#topol = \"topol.top\"\n",
    "topol = \"topol_correct.top\"\n",
    "index = 'index_correct.ndx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5078d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = md.load(traj,top=conf)\n",
    "idx=tr[0].top.select(\"name CA\")\n",
    "#idx=tr[0].top.select(\"element != H\")\n",
    "tr.superpose(tr[0],atom_indices=idx)\n",
    "geom = np.moveaxis(tr.xyz ,0,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb9186b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sparse and dense feture extensions of IC\n",
    "density = 2 # integer in [1, n_atoms-1]\n",
    "sparse_dists = asmsa.NBDistancesSparse(geom.shape[0], density=density)\n",
    "dense_dists = asmsa.NBDistancesDense(geom.shape[0])\n",
    "\n",
    "# mol = asmsa.Molecule(conf,topol)\n",
    "# mol = asmsa.Molecule(conf,topol,fms=[sparse_dists])\n",
    "mol = asmsa.Molecule(pdb=conf,top=topol,ndx=index,fms=[sparse_dists])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47653b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = mol.intcoord(geom).T\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a199f7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "molecule_shape = (X_train.shape[1],)\n",
    "latent_dim = 2\n",
    "prior = 'normal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f141112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder tuning\n",
    "\n",
    "def build_encoder(params=[(\"selu\", 32),\n",
    "                                 (\"selu\", 16),\n",
    "                                 (\"selu\", 8),\n",
    "                                 (\"linear\", None)]):\n",
    "    model = Sequential()\n",
    "    # input layer\n",
    "    model.add(Dense(params[0][1], input_dim=np.prod(molecule_shape), activation=params[0][0]))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    # hidden layers\n",
    "    model.add(Dense(params[1][1], activation=params[1][0]))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(params[2][1], activation=params[2][0]))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    #output layer\n",
    "    model.add(Dense(latent_dim, activation=params[3][0]))\n",
    "    mol = Input(shape=molecule_shape)\n",
    "    lowdim = model(mol)\n",
    "    return Model(mol, lowdim, name=\"Encoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cc79e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder tuning\n",
    "\n",
    "def build_decoder(params=[(\"selu\", 8),\n",
    "                                 (\"selu\", 16),\n",
    "                                 (\"selu\", 32),\n",
    "                                 (\"linear\", None)]):\n",
    "    model = Sequential()\n",
    "    model._name = \"Decoder\"\n",
    "    # input layer\n",
    "    model.add(Dense(params[0][1], input_dim=latent_dim, activation=params[0][0]))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    # hidden layers\n",
    "    model.add(Dense(params[1][1], activation=params[1][0]))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(params[2][1], activation=params[2][0]))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    # output layer\n",
    "    model.add(Dense(np.prod(molecule_shape), activation=params[3][0]))\n",
    "    model.add(Reshape(molecule_shape))\n",
    "    lowdim = Input(shape=(latent_dim,))\n",
    "    mol = model(lowdim)\n",
    "    return Model(lowdim, mol, name=\"Decoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92732646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# discriminator tuning\n",
    "\n",
    "def build_discriminator(params=[(None, 512),\n",
    "                                       (None, 256),\n",
    "                                       (None, 256),\n",
    "                                       (None, 1)]):\n",
    "    model = Sequential()\n",
    "    model._name = \"Discriminator\"\n",
    "    model.add(Flatten(input_shape=(latent_dim,)))\n",
    "    model.add(Dense(params[0][1]))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(params[1][1]))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(params[2][1]))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(params[3][1]))\n",
    "\n",
    "    mol = Input(shape=(latent_dim,))\n",
    "    validity = model(mol)\n",
    "    return Model(mol, validity, name=\"Discriminator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f8ba05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adversarial autoencoder\n",
    "\n",
    "class AAEModel(Model):\n",
    "    def __init__(self,enc,dec,disc,latent_dim,prior):\n",
    "        super().__init__()\n",
    "        self.enc = enc\n",
    "        self.dec = dec\n",
    "        self.disc = disc\n",
    "        self.lowdim = latent_dim\n",
    "        self.prior = prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e5d054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hypermodel\n",
    "\n",
    "class MyHyperModel(keras_tuner.HyperModel):\n",
    "    def build(self, hp):\n",
    "        enc = build_encoder()\n",
    "        dec = build_decoder()\n",
    "        disc = build_discriminator()\n",
    "        \n",
    "        return AAEModel(enc,dec,disc,latent_dim,prior)\n",
    "    \n",
    "\n",
    "    def fit(self, hp, model, X_train, callbacks=None, **kwargs):\n",
    "        # Convert the datasets to tf.data.Dataset.\n",
    "        \n",
    "        batch_size = hp.Int(\"batch_size\", 32, 128, step=32, default=64)\n",
    "#         batch_size = 256\n",
    "        \n",
    "        # create dataset\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(X_train)\n",
    "        dataset = dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "        \n",
    "        # set loss functions\n",
    "        ae_loss_fn = MeanSquaredError()\n",
    "        disc_loss_fn = BinaryCrossentropy(from_logits=True)\n",
    "        opt = Adam(0.0002,0.5)\n",
    "        \n",
    "        # The metric to track loss across training steps\n",
    "        epoch_loss_metric = keras.metrics.Mean()\n",
    "\n",
    "        # Function to run the train step.\n",
    "        @tf.function\n",
    "        def run_train_step(batch):\n",
    "            def _get_prior(name, shape):\n",
    "                if name == \"normal\":\n",
    "                    return tf.random.normal(shape=shape)\n",
    "                if name == \"uniform\":\n",
    "                    return tf.random.uniform(shape=shape)\n",
    "\n",
    "                raise ValueError(f\"Invalid prior type '{name}'. Choose from 'normal|uniform'\")\n",
    "\n",
    "            if isinstance(batch,tuple):\n",
    "                batch = batch[0]\n",
    "\n",
    "            batch_size = tf.shape(batch)[0]\n",
    "\n",
    "            # improve AE to reconstruct\n",
    "            with tf.GradientTape(persistent=True) as ae_tape:\n",
    "                reconstruct = model.dec(model.enc(batch))\n",
    "                ae_loss = ae_loss_fn(batch,reconstruct)\n",
    "\n",
    "            enc_grads = ae_tape.gradient(ae_loss, model.enc.trainable_weights)\n",
    "            opt.apply_gradients(zip(enc_grads,model.enc.trainable_weights))\n",
    "\n",
    "            dec_grads = ae_tape.gradient(ae_loss, model.dec.trainable_weights)\n",
    "            opt.apply_gradients(zip(dec_grads,model.dec.trainable_weights))\n",
    "\n",
    "            # improve discriminator\n",
    "            rand_low = _get_prior(model.prior, (batch_size, model.lowdim))\n",
    "            better_low = model.enc(batch)\n",
    "            low = tf.concat([rand_low,better_low],axis=0)\n",
    "\n",
    "            labels = tf.concat([tf.ones((batch_size,1)), tf.zeros((batch_size,1))], axis=0)\n",
    "            labels += 0.05 * tf.random.uniform(tf.shape(labels))\t# guide\n",
    "\n",
    "            with tf.GradientTape() as disc_tape:\n",
    "                pred = model.disc(low)\n",
    "                disc_loss = disc_loss_fn(labels,pred)\n",
    "\n",
    "            disc_grads = disc_tape.gradient(disc_loss,model.disc.trainable_weights)\n",
    "            opt.apply_gradients(zip(disc_grads,model.disc.trainable_weights))\n",
    "\n",
    "            # teach encoder to cheat\n",
    "            alltrue = tf.ones((batch_size,1))\n",
    "\n",
    "            with tf.GradientTape() as cheat_tape:\n",
    "                cheat = model.disc(model.enc(batch))\n",
    "                cheat_loss = disc_loss_fn(alltrue,cheat)\n",
    "\n",
    "            cheat_grads = cheat_tape.gradient(cheat_loss,model.enc.trainable_weights)\n",
    "            opt.apply_gradients(zip(cheat_grads,model.enc.trainable_weights))\n",
    "            \n",
    "            epoch_loss_metric.update_state(ae_loss)\n",
    "\n",
    "        # Assign the model to the callbacks.\n",
    "        for callback in callbacks:\n",
    "            callback.model = model\n",
    "\n",
    "        # Record the best validation loss value\n",
    "        best_epoch_loss = float(\"inf\")\n",
    "\n",
    "        # The custom training loop.\n",
    "        for epoch in range(2):\n",
    "            print(f\"Epoch: {epoch}\")\n",
    "\n",
    "            # Iterate the training data to run the training step.\n",
    "            for batch in dataset:\n",
    "                run_train_step(batch)\n",
    "\n",
    "            # Calling the callbacks after epoch.\n",
    "            epoch_loss = float(epoch_loss_metric.result().numpy())\n",
    "            for callback in callbacks:\n",
    "                # The \"my_metric\" is the objective passed to the tuner.\n",
    "                callback.on_epoch_end(epoch, logs={\"ae_loss\": epoch_loss})\n",
    "            epoch_loss_metric.reset_states()\n",
    "\n",
    "            print(f\"Epoch loss: {epoch_loss}\")\n",
    "            best_epoch_loss = min(best_epoch_loss, epoch_loss)\n",
    "\n",
    "        # Return the evaluation metric value.\n",
    "        return best_epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed79e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = keras_tuner.RandomSearch(\n",
    "    objective=keras_tuner.Objective(\"ae_loss\", \"min\"),\n",
    "    max_trials=10,\n",
    "    hypermodel=MyHyperModel(),\n",
    "    directory=\"results\",\n",
    "    project_name=\"custom_training\",\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e58877",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca463d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.results_summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
