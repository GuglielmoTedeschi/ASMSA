{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6851ea37",
   "metadata": {},
   "source": [
    "# ASMSA: Tune AAE model hyperparameters\n",
    "\n",
    "**Previous step**\n",
    "- [prepare.ipynb](prepare.ipynb): Download and sanity check input files\n",
    "\n",
    "**Next steps**\n",
    "- [train.ipynb](train.ipynb): Use results of previous tuning in more thorough training\n",
    "- [md.ipynb](md.ipynb): Use a trained model in MD simulation with Gromacs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235ca1f6",
   "metadata": {},
   "source": [
    "## Notebook setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91559377-60e1-421a-a51e-5e78f0c1b99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "threads = 2\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS']=str(threads)\n",
    "import tensorflow as tf\n",
    "\n",
    "# PyTorch favours OMP_NUM_THREADS in environment\n",
    "import torch\n",
    "\n",
    "# Tensorflow needs explicit cofig calls\n",
    "tf.config.threading.set_inter_op_parallelism_threads(threads)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b70ab11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mdtraj as md\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "from tensorflow import keras\n",
    "import keras_tuner\n",
    "import asmsa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaba1b8f-d0ff-4264-9712-d06ad9ac964f",
   "metadata": {},
   "source": [
    "## Input files\n",
    "\n",
    "All input files are prepared (up- or downloaded) in [prepare.ipynb](prepare.ipynb). \n",
    "\n",
    "This is for demonstration purpose, in real use the inputs should be placed here, and _conf, traj, topol, index_ variables set to their filenames names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd3c43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input files\n",
    "\n",
    "# input conformation\n",
    "#conf = \"alaninedipeptide_H.pdb\"\n",
    "conf = \"trpcage_correct.pdb\"\n",
    "\n",
    "# input trajectory\n",
    "# atom numbering must be consistent with {conf}\n",
    "\n",
    "#traj = \"alaninedipeptide_reduced.xtc\"\n",
    "traj = \"trpcage_red.xtc\"\n",
    "\n",
    "# input topology\n",
    "# expected to be produced with \n",
    "#    gmx pdb2gmx -f {conf} -p {topol} -n {index} -o {gro}\n",
    "\n",
    "# Gromacs changes atom numbering, the index file must be generated and used as well\n",
    "# gro file is used to generate inverse indexing for plumed.dat\n",
    "\n",
    "#topol = \"topol.top\"\n",
    "topol = \"topol_correct.top\"\n",
    "index = 'index_correct.ndx'\n",
    "gro = 'trpcage_correct.gro'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0100f507",
   "metadata": {},
   "source": [
    "## Internal coordinates computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef96d527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trajectory, it should report expected numbers of frames and atoms/residua\n",
    "\n",
    "tr = md.load(traj,top=conf)\n",
    "idx=tr[0].top.select(\"name CA\")\n",
    "\n",
    "# for trivial cases like Ala-Ala, where superposing on CAs fails\n",
    "#idx=tr[0].top.select(\"element != H\")\n",
    "\n",
    "tr.superpose(tr[0],atom_indices=idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc294831-a599-4874-b765-ac86d7f466be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshuffle the geometry to get frame last so that we can use vectorized calculations\n",
    "\n",
    "geom = np.moveaxis(tr.xyz ,0,-1)\n",
    "geom.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16daa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare internal coordinates computation. There are multiple options, see prepare.ipynb, and adjust here accordingly\n",
    "\n",
    "density = 2 # integer in [1, n_atoms-1]\n",
    "\n",
    "sparse_dists = asmsa.NBDistancesSparse(geom.shape[0], density=density)\n",
    "mol = asmsa.Molecule(pdb=conf,top=topol,ndx=index,fms=[sparse_dists])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a5bb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute on the actual input \n",
    "\n",
    "X_train = mol.intcoord(geom).T\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e6a129-5c6a-4a5c-8f77-31bbb05f5c49",
   "metadata": {},
   "source": [
    "## Toy model\n",
    "\n",
    "Train the AAE model with default hyperparameter values and only few epochs.\n",
    "\n",
    "Just test, it is not expected to yield anything useful. This section can be skipped.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aafcb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices(X_train).shuffle(2048).batch(64,drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63949ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "testm = asmsa.AAEModel((X_train.shape[1],))\n",
    "testm.compile()\n",
    "testm.fit(train_ds,epochs=10,callbacks=[asmsa.VisualizeCallback(testm,inputs=X_train,figsize=(12,3),freq=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd941556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the model configuration\n",
    "# testm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b36de9-52a3-4ff1-9726-5a0f5219ffb7",
   "metadata": {},
   "source": [
    "## Define hyperpararameter ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900bb5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter ranges are specified in terms of a lambda function, expecting keras_tuner.HyperParameters instance as parameter\n",
    "# The function is passed as a callback to asmsa.AAEHyperModel conststructor subsequently\n",
    "\n",
    "full_hp = lambda hp: \\\n",
    "    {\n",
    "        'activation' : hp.Choice('activation', ['relu', 'gelu', 'selu']),\n",
    "        'ae_neuron_number_seed' : hp.Int(\"ae_neuron_number_seed\", 32, 224, step=64),\n",
    "        'disc_neuron_number_seed' : hp.Int(\"disc_neuron_number_seed\", 32, 224, step=64),\n",
    "        'ae_number_of_layers' : hp.Int(\"ae_number_of_layers\", 2, 3, step=1),\n",
    "        'disc_number_of_layers' : hp.Int(\"disc_number_of_layers\", 2, 3, step=1),\n",
    "        'batch_size' : hp.Int(\"batch_size\", 32, 256, step=64),\n",
    "        'optimizer' : hp.Choice('optimizer', ['Adam', 'SGD', 'RMSProp', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam','Ftrl' ]),\n",
    "        'ae_loss_fn' : hp.Choice('ae_loss_fn', [ 'MeanSquaredError', 'Huber']),\n",
    "        'disc_loss_fn' : hp.Choice('disc_loss_fn', ['BinaryCrossentropy'])\n",
    "    }\n",
    "\n",
    "medium_hp = lambda hp: \\\n",
    "     {\n",
    "        'activation' : hp.Choice('activation', ['relu', 'gelu']),\n",
    "        'ae_neuron_number_seed' : hp.Int(\"ae_neuron_number_seed\", 32, 224, step=64),\n",
    "        'disc_neuron_number_seed' : hp.Int(\"disc_neuron_number_seed\", 32, 224, step=64),\n",
    "        'ae_number_of_layers' : hp.Int(\"ae_number_of_layers\", 2, 2),\n",
    "        'disc_number_of_layers' : hp.Int(\"disc_number_of_layers\", 2, 2),\n",
    "        'batch_size' : hp.Int(\"batch_size\", 64, 128, step=64),\n",
    "        'optimizer' : hp.Choice('optimizer', ['Adam']),\n",
    "        'ae_loss_fn' : hp.Choice('ae_loss_fn', ['MeanSquaredError']),\n",
    "        'disc_loss_fn' : hp.Choice('disc_loss_fn', ['BinaryCrossentropy'])\n",
    "    }\n",
    "\n",
    "tiny_hp = lambda hp: \\\n",
    "     {\n",
    "        'activation' : hp.Choice('activation', ['relu']),\n",
    "        'ae_neuron_number_seed' : hp.Int(\"ae_neuron_number_seed\", 32, 64, step=32),\n",
    "        'disc_neuron_number_seed' : hp.Int(\"disc_neuron_number_seed\", 32, 32),\n",
    "        'ae_number_of_layers' : hp.Int(\"ae_number_of_layers\", 2, 2),\n",
    "        'disc_number_of_layers' : hp.Int(\"disc_number_of_layers\", 2, 2),\n",
    "        'batch_size' : hp.Int(\"batch_size\", 64, 128, step=64),\n",
    "        'optimizer' : hp.Choice('optimizer', ['Adam']),\n",
    "        'ae_loss_fn' : hp.Choice('ae_loss_fn', ['MeanSquaredError']),\n",
    "        'disc_loss_fn' : hp.Choice('disc_loss_fn', ['BinaryCrossentropy'])\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3092061-7a90-4a37-b2b4-4606a9fadef1",
   "metadata": {},
   "source": [
    "## Sequential hyperparameter tuning\n",
    "\n",
    "This is robust, it does not require Kubernetes environment for additional job submission. On the other hand, it is slow accordingly.\n",
    "\n",
    "**Skip to the next section if you run the notebook in our recommended setup in Kubernets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a2080a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just testing numbers of epochs and hyperparameter setting trials\n",
    "# Don't expect anything meaningful\n",
    "\n",
    "trials=6\n",
    "epochs=5\n",
    "\n",
    "tuner = keras_tuner.RandomSearch(\n",
    "    max_trials=trials,\n",
    "    hypermodel=asmsa.AAEHyperModel((X_train.shape[1],),hpfunc=tiny_hp),\n",
    "    objective=keras_tuner.Objective(\"score\", direction=\"min\"),\n",
    "    directory=\"./results\",\n",
    "    project_name=\"Random\",\n",
    "    overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6141ae3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(X_train,epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53d4268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# too verbose output, commented out by default\n",
    "# tuner.search_space_summary(extended=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe57cf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cf0999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# access to the results, values sorted from the best to the worst\n",
    "best_hp = tuner.get_best_hyperparameters(num_trials=trials)\n",
    "best_hp[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67fd3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to use the tuned hyperparamters in model training \n",
    "# visualize results \n",
    "\n",
    "trial=0\n",
    "testm = asmsa.AAEModel((X_train.shape[1],),hp=best_hp[trial].values)\n",
    "ds = tf.data.Dataset.from_tensor_slices(X_train).shuffle(2048).batch(best_hp[trial].values['batch_size'],drop_remainder=True)\n",
    "testm.compile()\n",
    "testm.fit(ds,epochs=20,callbacks=[asmsa.VisualizeCallback(testm,freq=5,inputs=X_train,figsize=(12,3))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587b8ad6-7629-4d97-a91f-27968b045925",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = asmsa.Visualizer(figsize=(12,4))\n",
    "visualizer.make_visualization(testm.call_enc(X_train).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7401d7d-35b1-43db-9446-7f1e3714994a",
   "metadata": {},
   "source": [
    "## Parallel hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e992cd-368e-4162-9eeb-8295b9478e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, this is the real stuff\n",
    "# medium settings known to be working for trpcage\n",
    "\n",
    "epochs=200\n",
    "trials=50\n",
    "hpfunc=medium_hp\n",
    "\n",
    "# testing only\n",
    "#epochs=8\n",
    "#trials=6\n",
    "#hpfunc=tiny_hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1ee1d0-ae41-4e3c-91de-9910eca8b3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of parallel workers, each runs a single trial at time\n",
    "# balance between resource availability and size of the problem\n",
    "# currently each slave runs on 4 cores and 4 GB RAM (hardcoded in src/asmsa/tunewrapper.py)\n",
    "\n",
    "slaves=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775255db-6e7e-4042-9e29-4a6a38705459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXX: Kubernetes magic: find out names of container image and volume\n",
    "# check the result, it can go wrong\n",
    "\n",
    "with open('IMAGE') as img:\n",
    "    image=img.read().rstrip()\n",
    "\n",
    "import re\n",
    "mnt=os.popen('mount | grep /home/jovyan').read()\n",
    "pvcid=re.search('pvc-[0-9a-z-]+',mnt).group(0)\n",
    "pvc=os.popen(f'kubectl get pvc | grep {pvcid} | cut -f1 -d\" \"').read().rstrip()\n",
    "\n",
    "print(f\"\"\"\\\n",
    "image: {image}\n",
    "volume: {pvc}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d954868-09bd-427a-ae6a-4334c6ab55d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python wrapper around scripts that prepare and execute parellel Keras Tuner in Kubernetes\n",
    "\n",
    "wrapper = asmsa.TuneWrapper(hpfunc=medium_hp,output='best.txt',epochs=epochs,trials=trials,pdb=conf,top=topol,xtc=traj,ndx=index, pvc=pvc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153d894e-b03f-490a-8c8d-cd5310ab2ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary but destructive cleanup before hyperparameter tuning\n",
    "\n",
    "# DON'T RUN THIS CELL BLINDLY\n",
    "# it kills any running processes including the workers, and it purges previous results\n",
    "\n",
    "!kubectl delete job/tuner\n",
    "!kill $(ps ax | grep tuning.py | awk '{print $1}')\n",
    "!rm -rf results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff538642-3ee1-4205-bd2c-7900347d0596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backup previous results; do so only if you want to\n",
    "\n",
    "!mv best.txt best.txtO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666446de-eb9f-487e-860a-6efadd2ae065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the master (chief) of tuners in background\n",
    "# the computation takes rather long, this is a more robust approach then keeping it in the notebook\n",
    "\n",
    "wrapper.master_start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f576a47a-23c6-46b8-b0ba-d184e8598390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# therefore one should check the status ocassionally; it should show a tuning.py process running\n",
    "print(wrapper.master_status())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb5eb38-fb24-4236-8349-7f694bea3f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spawn the requested number of workers as separate Kubernetes job with several pods \n",
    "# they receive work from \n",
    "\n",
    "wrapper.workers_start(num=slaves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f752369-13d2-4ed0-be1e-115b446dc35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This status should show {slaves} number of pods, all of them start in Pending state, and follow through ContainerCreating \n",
    "# to Running, and Completed finally\n",
    "\n",
    "# This takes time, minutes to hours depending on size of the model, number of trials, and number of slaves\n",
    "# Run this cell repeatedly, waiting until all the pods are completed\n",
    "\n",
    "wrapper.workers_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bed4a3-ddd4-48c0-9794-aacb98b3db16",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
