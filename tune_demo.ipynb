{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6851ea37",
   "metadata": {},
   "source": [
    "# ASMSA: Tune AAE model hyperparameters\n",
    "\n",
    "**Previous step**\n",
    "- [prepare.ipynb](prepare.ipynb): Download and sanity check input files\n",
    "\n",
    "**Next steps**\n",
    "- [train.ipynb](train.ipynb): Use results of previous tuning in more thorough training\n",
    "- [md.ipynb](md.ipynb): Use a trained model in MD simulation with Gromacs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235ca1f6",
   "metadata": {},
   "source": [
    "## Notebook setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91559377-60e1-421a-a51e-5e78f0c1b99b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "threads = 2\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS']=str(threads)\n",
    "import tensorflow as tf\n",
    "\n",
    "# PyTorch favours OMP_NUM_THREADS in environment\n",
    "import torch\n",
    "\n",
    "# Tensorflow needs explicit cofig calls\n",
    "tf.config.threading.set_inter_op_parallelism_threads(threads)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b70ab11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mdtraj as md\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "from tensorflow import keras\n",
    "import keras_tuner\n",
    "import asmsa\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaba1b8f-d0ff-4264-9712-d06ad9ac964f",
   "metadata": {},
   "source": [
    "## Input files\n",
    "\n",
    "All input files are prepared (up- or downloaded) in [prepare.ipynb](prepare.ipynb). \n",
    "\n",
    "This is for demonstration purpose, in real use the inputs should be placed here, and _conf, traj, topol, index_ variables set to their filenames names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd3c43e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define input files\n",
    "\n",
    "# input conformation\n",
    "#conf = \"alaninedipeptide_H.pdb\"\n",
    "conf = \"trpcage_correct.pdb\"\n",
    "\n",
    "# input trajectory\n",
    "# atom numbering must be consistent with {conf}\n",
    "\n",
    "#traj = \"alaninedipeptide_reduced.xtc\"\n",
    "traj = \"trpcage_red.xtc\"\n",
    "\n",
    "# input topology\n",
    "# expected to be produced with \n",
    "#    gmx pdb2gmx -f {conf} -p {topol} -n {index} -o {gro}\n",
    "\n",
    "# Gromacs changes atom numbering, the index file must be generated and used as well\n",
    "# gro file is used to generate inverse indexing for plumed.dat\n",
    "\n",
    "#topol = \"topol.top\"\n",
    "topol = \"topol_correct.top\"\n",
    "index = 'index_correct.ndx'\n",
    "gro = 'trpcage_correct.gro'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0100f507",
   "metadata": {},
   "source": [
    "## Internal coordinates computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef96d527",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the trajectory, it should report expected numbers of frames and atoms/residua\n",
    "\n",
    "tr = md.load(traj,top=conf)\n",
    "idx=tr[0].top.select(\"name CA\")\n",
    "\n",
    "# for trivial cases like Ala-Ala, where superposing on CAs fails\n",
    "#idx=tr[0].top.select(\"element != H\")\n",
    "\n",
    "tr.superpose(tr[0],atom_indices=idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc294831-a599-4874-b765-ac86d7f466be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reshuffle the geometry to get frame last so that we can use vectorized calculations\n",
    "\n",
    "geom = np.moveaxis(tr.xyz ,0,-1)\n",
    "geom.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16daa6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare internal coordinates computation. There are multiple options, see prepare.ipynb, and adjust here accordingly\n",
    "\n",
    "density = 2 # integer in [1, n_atoms-1]\n",
    "\n",
    "sparse_dists = asmsa.NBDistancesSparse(geom.shape[0], density=density)\n",
    "mol = asmsa.Molecule(pdb=conf,top=topol,ndx=index,fms=[sparse_dists])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a5bb18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# compute on the actual input \n",
    "\n",
    "X_train = mol.intcoord(geom).T\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aafcb19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices(X_train).shuffle(2048).batch(64,drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3092061-7a90-4a37-b2b4-4606a9fadef1",
   "metadata": {},
   "source": [
    "## Sequential hyperparameter tuning\n",
    "\n",
    "This is robust, it does not require Kubernetes environment for additional job submission. On the other hand, it is slow accordingly.\n",
    "\n",
    "**Skip to the next section if you run the notebook in our recommended setup in Kubernets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4daba3b-ea22-414c-ada4-d02353454b05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "medium_hp = {\n",
    "    'activation' : ['relu','gelu'],\n",
    "    'ae_neuron_number_seed' : [32,96,128],\n",
    "    'disc_neuron_number_seed' : [32,96],\n",
    "    'ae_number_of_layers' : [2,2],\n",
    "    'disc_number_of_layers' : [3,3],\n",
    "    'batch_size' : [64,128,256],\n",
    "    'optimizer' : ['Adam'],\n",
    "    'learning_rate' : 0.0002,\n",
    "    'ae_loss_fn' : ['MeanSquaredError'],\n",
    "    'disc_loss_fn' : ['BinaryCrossentropy']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a2080a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Just testing numbers of epochs and hyperparameter setting trials\n",
    "# Don't expect anything meaningful\n",
    "\n",
    "trials=3\n",
    "epochs=15\n",
    "results_dir=datetime.today().strftime(\"%m%d%Y-%H%M%S\")\n",
    "\n",
    "os.environ['START_TIME']=results_dir\n",
    "tuner = keras_tuner.RandomSearch(\n",
    "    max_trials=trials,\n",
    "    hypermodel=asmsa.AAEHyperModel((X_train.shape[1],),hpfunc=medium_hp),\n",
    "    objective=keras_tuner.Objective(\"score\", direction=\"min\"),\n",
    "    directory=\"./results\",\n",
    "    project_name=\"Random\",\n",
    "    overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6141ae3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tuner.search(X_train,epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df31d980-e476-4956-9151-66d4433c3fa6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from asmsa.tuning_analyzer import TuningAnalyzer\n",
    "\n",
    "# Create analyzer object that analyses results of tuning\n",
    "# By default it is the latest tuning, but can by configured with tuning flag\n",
    "#  which is set to the directory of the tuning, e.g tuning='ASMSA_visualization/05092023-135249'\n",
    "analyzer = TuningAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21acdf17-9123-44ff-8b30-12c3115064ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get sorted hyperparameters by score, by default 10 best HP\n",
    "analyzer.get_best_hp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a540e35-2d13-44b2-bc14-86427ded9338",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Matplotlib visualization - not recommended way, does not look that good and does not scale \n",
    "#  that well but at least the colors are consistent accross measures. After more work could look better\n",
    "# There is an option to reduce amount of plots num_trials and can choose only one\n",
    "# By default choosing the latest tuning (can be changed in definition of TuingAnalyzer)\n",
    "analyzer.visualize_tuning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2829d890-6ab3-490a-b6d4-308ad809492a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Recommended option via tensorboard. This function populates TB event\n",
    "#  which can be viewed in native way via tensorboard. By default chooses\n",
    "#  latest tuning and populates into its directory _TB, e.g:\n",
    "#  ASMSA_visualization/05092023-135249/_TB\n",
    "analyzer.populate_TB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cd9b4a-f5cd-4627-b379-51836ae3f9ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ASMSA_visualization/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7401d7d-35b1-43db-9446-7f1e3714994a",
   "metadata": {},
   "source": [
    "## Parallel hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e992cd-368e-4162-9eeb-8295b9478e15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Finally, this is the real stuff\n",
    "# medium settings known to be working for trpcage\n",
    "\n",
    "epochs=15\n",
    "trials=3\n",
    "hpfunc=medium_hp\n",
    "\n",
    "# testing only\n",
    "#epochs=8\n",
    "#trials=6\n",
    "#hpfunc=tiny_hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1ee1d0-ae41-4e3c-91de-9910eca8b3c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# number of parallel workers, each runs a single trial at time\n",
    "# balance between resource availability and size of the problem\n",
    "# currently each slave runs on 4 cores and 4 GB RAM (hardcoded in src/asmsa/tunewrapper.py)\n",
    "\n",
    "slaves=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775255db-6e7e-4042-9e29-4a6a38705459",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# XXX: Kubernetes magic: find out names of container image and volume\n",
    "# check the result, it can go wrong\n",
    "\n",
    "with open('IMAGE') as img:\n",
    "    image=img.read().rstrip()\n",
    "\n",
    "import re\n",
    "mnt=os.popen('mount | grep /home/jovyan').read()\n",
    "pvcid=re.search('pvc-[0-9a-z-]+',mnt).group(0)\n",
    "pvc=os.popen(f'kubectl get pvc | grep {pvcid} | cut -f1 -d\" \"').read().rstrip()\n",
    "\n",
    "print(f\"\"\"\\\n",
    "image: {image}\n",
    "volume: {pvc}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d954868-09bd-427a-ae6a-4334c6ab55d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Python wrapper around scripts that prepare and execute parellel Keras Tuner in Kubernetes\n",
    "from asmsa.tunewrapper import TuneWrapper\n",
    "\n",
    "wrapper = TuneWrapper(hpfunc=hpfunc,output='best.txt',epochs=epochs,trials=trials,pdb=conf,top=topol,xtc=traj,ndx=index, pvc=pvc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153d894e-b03f-490a-8c8d-cd5310ab2ab8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Necessary but destructive cleanup before hyperparameter tuning\n",
    "\n",
    "# DON'T RUN THIS CELL BLINDLY\n",
    "# it kills any running processes including the workers, and it purges previous results\n",
    "\n",
    "!kubectl delete job/tuner\n",
    "!kill $(ps ax | grep tuning.py | awk '{print $1}')\n",
    "!rm -rf results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff538642-3ee1-4205-bd2c-7900347d0596",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# backup previous results; do so only if you want to\n",
    "\n",
    "!mv best.txt best.txtO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666446de-eb9f-487e-860a-6efadd2ae065",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# start the master (chief) of tuners in background\n",
    "# the computation takes rather long, this is a more robust approach then keeping it in the notebook\n",
    "\n",
    "wrapper.master_start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f576a47a-23c6-46b8-b0ba-d184e8598390",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# therefore one should check the status ocassionally; it should show a tuning.py process running\n",
    "print(wrapper.master_status())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb5eb38-fb24-4236-8349-7f694bea3f3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# spawn the requested number of workers as separate Kubernetes job with several pods \n",
    "# they receive work from \n",
    "\n",
    "wrapper.workers_start(num=slaves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f752369-13d2-4ed0-be1e-115b446dc35c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This status should show {slaves} number of pods, all of them start in Pending state, and follow through ContainerCreating \n",
    "# to Running, and Completed finally\n",
    "\n",
    "# This takes time, minutes to hours depending on size of the model, number of trials, and number of slaves\n",
    "# Run this cell repeatedly, waiting until all the pods are completed\n",
    "\n",
    "wrapper.workers_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec269b7-0096-4861-9c86-3e7f87bce9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same steps for analysis as with serial tuning\n",
    "analyzer = TuningAnalyzer()\n",
    "analyzer.get_best_hp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4536d3cf-109b-4719-a88f-42031becd2b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We can choose output dir for TB event this time\n",
    "analyzer.populate_TB(out_dir='test2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1683eb5f-6d7e-4b02-89f5-ea6f7c8b0130",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Might need to kill previous tensorboard instance to change logdir\n",
    "!pkill -f 'tensorboard'\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22452d5-a5ac-4ccf-8231-c9b1a2835613",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
