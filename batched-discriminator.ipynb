{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bb6358-51d5-4eb5-abf1-b42ee2445320",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras as k\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import asmsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd33fc0-0bfd-4c1b-98ca-299e0c19ed16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _Hist2D(x,min=-1.,max=1.,bins=10):\n",
    "    zeros = tf.zeros((bins,bins),tf.float32)\n",
    "# XXX: bounds\n",
    "    idx = (x - min) * (1./(max - min))\n",
    "    idx = tf.cast(idx * bins,tf.int32)\n",
    "\n",
    "    ones = tf.ones(tf.shape(x)[0])\n",
    "    return tf.tensor_scatter_nd_add(zeros,idx,ones) / tf.cast(tf.shape(x)[0],tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7320670e-762f-4116-9869-4550d2f067e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdef709-8265-4f8b-ae2b-86b86502cc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior=tfp.distributions.MultivariateNormalDiag(loc=[0.,0.])\n",
    "s = prior.sample(1000)\n",
    "h = _Hist2D(s,min=-3,max=3,bins=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d54e76c-bc69-4e42-a2ea-9e639bb8c938",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(h)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4475dc9a-be1a-4b1d-a76b-b271a67a5222",
   "metadata": {},
   "outputs": [],
   "source": [
    "s[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bbd59d-67c6-463c-b4cc-0e2324fb78a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924ccef3-f8ef-46b5-a009-7790dc581f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _TransposeLayer(k.layers.Layer):\n",
    "    def __init__(self,batch_size=None):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def call(self,inputs):\n",
    "        if self.batch_size:\n",
    "            return tf.transpose(tf.reshape(inputs,[self.batch_size,inputs.shape[1]]))\n",
    "        return tf.transpose(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb178de-5202-4647-82e6-a8d1f0128279",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = k.Input(shape=(2,))\n",
    "out = k.layers.Dense(10)(inp)\n",
    "out = _TransposeLayer(batch_size=3)(out)\n",
    "out = k.layers.Dense(10)(out)\n",
    "#out = _TransposeLayer()(out)\n",
    "out = k.layers.Dense(1)(out)\n",
    "out = _TransposeLayer()(out)\n",
    "out = k.layers.Dense(1)(out)\n",
    "mod = k.Model(inp,out)\n",
    "mod.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4453edd-7561-4f8b-ab73-96c7826d6eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c9f4e1-3816-4bc5-99ea-07985cbeed4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.call(tf.constant([[1,2],[3,4],[5,6]],tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01763401-3459-472a-ac67-c31808ff2ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "_default_hp = {\n",
    "    'activation' : 'gelu',\n",
    "    'ae_loss_fn': 'MeanSquaredError',\n",
    "    'optimizer': 'Adam',\n",
    "    'learning_rate' : 0.0002,\n",
    "    'kde_sigma': 0.01,\n",
    "    'kl_weight': 1.0,\n",
    "}\n",
    "\n",
    "@tf.function\n",
    "def _KDEProb(ref,qry,sigma=1.):\n",
    "    rsigma2 = -1./(2.*sigma*sigma)\n",
    "\n",
    "    refs = tf.concat([tf.slice(tf.shape(qry),[0],[1]),tf.shape(ref)],0)\n",
    "    qrys = tf.concat([tf.slice(tf.shape(ref),[0],[1]),tf.shape(qry)],0)\n",
    " \n",
    "    mref = tf.broadcast_to(ref,refs)\n",
    "    mqry = tf.broadcast_to(qry,qrys)\n",
    "    mqry = tf.transpose(mqry,[1,0,2]) # XXX: exactly 1D shape of latent space dim\n",
    "\n",
    "    dist2 = tf.math.reduce_sum(\n",
    "        tf.math.pow(mref-mqry,2),\n",
    "        axis=2\n",
    "    )\n",
    "    kdes = tf.exp(dist2 * rsigma2)\n",
    "    kde = tf.reduce_mean(kdes,axis=1)\\\n",
    "        * tf.math.pow(tf.constant(2.*math.pi), tf.cast(tf.shape(ref)[1],tf.float32) * -.5)\\\n",
    "        / sigma\n",
    "    return kde\n",
    "\n",
    "class BatchedAAEModel(k.models.Model):\n",
    "    def __init__(self,mol_shape,latent_dim=2,ae_layers=[64,32,8],disc_layers=[8,16,8],bins=10,\n",
    "                 prior=tfp.distributions.MultivariateNormalDiag(loc=[0.,0.]),hp=_default_hp):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.hp = hp\n",
    "        self.latent_dim = latent_dim\n",
    "        self.prior = prior\n",
    "        self.bins = bins\n",
    "\n",
    "        inp = k.Input(shape = mol_shape)\n",
    "        out = inp\n",
    "\n",
    "        for i,n in enumerate(ae_layers):\n",
    "            out = k.layers.Dense(n,activation=hp['activation'],name=f'enc_{i}')(out)\n",
    "            out = k.layers.BatchNormalization(momentum=0.8,name=f'enc_bn_{i}')(out)\n",
    "\n",
    "        out = k.layers.Dense(latent_dim,name='enc_out')(out)\n",
    "        latent = out\n",
    "\n",
    "        for i,n in enumerate(reversed(ae_layers)):\n",
    "            out = k.layers.Dense(n,activation=hp['activation'],name=f'dec_{i}')(out)\n",
    "            out = k.layers.BatchNormalization(momentum=0.8,name=f'dec_bn_{i}')(out)\n",
    "\n",
    "        out = k.layers.Dense(mol_shape,activation=hp['activation'],name='dec_out')(out)\n",
    "\n",
    "        self.enc = k.Model(inputs=inp,outputs=latent)\n",
    "        self.dec = k.Model(inputs=latent,outputs=out)\n",
    "\n",
    "        \"\"\"out = _TransposeLayer(batch_size=batch_size)(latent)\n",
    "        for i,n in enumerate(disc_layers):\n",
    "            out = k.layers.Dense(n,activation=hp['activation'],name=f'disc_{i}')(out)\n",
    "            out = k.layers.BatchNormalization(momentum=0.8,name=f'disc_bn_{i}')(out)\n",
    "            out = _TransposeLayer()(out)\n",
    "\n",
    "        out = k.layers.Dense(1,activation=hp['activation'])(out)\n",
    "        out = _TransposeLayer()(out)\n",
    "        out = k.layers.Dense(1,activation=hp['activation'])(out)\n",
    "\n",
    "        self.disc = k.Model(inputs=latent,outputs=out)\"\"\"\n",
    "        \n",
    "        inp = k.Input(shape=(bins,bins,1))\n",
    "        out = k.layers.Conv2D(10,5,padding='same',activation=hp['activation'])(inp)\n",
    "        out = k.layers.Conv2D(10,5,padding='same',activation=hp['activation'])(out)\n",
    "        out = k.layers.Flatten()(out)\n",
    "        out = k.layers.Dense(50,activation=hp['activation'])(out)\n",
    "        out = k.layers.Dense(1,activation=hp['activation'])(out)\n",
    "        self.disc = k.Model(inputs=inp,outputs=out)\n",
    "        \n",
    "    def compile(self,optimizer=None,lossfn=k.losses.MeanSquaredError()):\n",
    "        if optimizer is None:\n",
    "            optimizer = self.hp['optimizer']\n",
    "\n",
    "        if isinstance(optimizer,str):\n",
    "            optimizer = k.optimizers.legacy.__dict__[optimizer]\n",
    "\n",
    "        super().compile(optimizer = optimizer(learning_rate=self.hp['learning_rate']))\n",
    "        self.ae_weights = self.enc.trainable_weights + self.dec.trainable_weights\n",
    "        self.lossfn = lossfn\n",
    "        self.discloss = k.losses.CategoricalCrossentropy(from_logits=True)\n",
    "   \n",
    "    @tf.function\n",
    "    def train_step(self,batch):\n",
    "        prob_shift = 1e-15\n",
    "        \n",
    "        if isinstance(batch,tuple):\n",
    "            batch = batch[0]\n",
    "\n",
    "        with tf.GradientTape() as aetape:\n",
    "            out = self.dec(self.enc(batch))\n",
    "            loss = self.lossfn(batch,out)\n",
    "\n",
    "        ae_grad = aetape.gradient(loss,self.ae_weights)\n",
    "        self.optimizer.apply_gradients(zip(ae_grad,self.ae_weights))\n",
    "\n",
    "        prior_sample = self.prior.sample(tf.shape(batch)[0])\n",
    "\n",
    "        latent = self.enc(batch)\n",
    "\n",
    "        \"\"\"with tf.GradientTape() as neg_tape:\n",
    "            neg = self.disc(latent)\n",
    "\n",
    "        neg_grad = neg_tape.gradient(neg,self.disc.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(neg_grad,self.disc.trainable_weights))\n",
    "\n",
    "        with tf.GradientTape() as pos_tape:\n",
    "            pos = -self.disc(prior_sample)\n",
    "\n",
    "        pos_grad = pos_tape.gradient(pos,self.disc.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(pos_grad,self.disc.trainable_weights))\"\"\"\n",
    "\n",
    "        disc_batch = tf.stack([_Hist2D(latent,bins=self.bins),_Hist2D(prior_sample,bins=self.bins)],axis=0)\n",
    "        with tf.GradientTape() as disc_tape:\n",
    "            out = self.disc(disc_batch)\n",
    "            disc_loss = self.discloss(tf.constant([0,1],tf.float32),out)\n",
    "\n",
    "        disc_grad = disc_tape.gradient(disc_loss,self.disc.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(disc_grad,self.disc.trainable_weights))\n",
    "\n",
    "        \"\"\"with tf.GradientTape() as cheat_tape:\n",
    "            cheat = self.disc(tf.reshape(_Hist2D(self.enc(batch),bins=self.bins),(1,self.bins,self.bins)))\n",
    "            cheat_loss = self.discloss(tf.constant([1],tf.float32),cheat)\n",
    "\n",
    "        cheat_grad = cheat_tape.gradient(cheat_loss,self.enc.trainable_weights)\n",
    "        print(cheat_grad)\n",
    "        self.optimizer.apply_gradients(zip(cheat_grad,self.enc.trainable_weights))\"\"\"\n",
    "\n",
    "        with tf.GradientTape() as t:\n",
    "            out = self.dec(self.enc(batch))\n",
    "            loss = self.\n",
    "        \n",
    "        return { 'AE loss' : loss, 'disc': disc_loss, 'cheat disc': cheat_loss }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d03aac-d454-42fb-b6e9-dcd4285a180a",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf='alaninedipeptide_H.pdb'\n",
    "gro='aladip_H.gro'\n",
    "topol='aladip_H.top'\n",
    "index=None\n",
    "traj='alaninedipeptide_reduced.xtc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24aa63b1-8a6d-4f35-8726-919fed4a7d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mdtraj as md\n",
    "import nglview as nv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8b8a63-888d-4e62-accd-5a3602533097",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tr = md.load(traj,top=conf)\n",
    "idx=tr[0].top.select(\"name CA\")\n",
    "\n",
    "# for trivial cases like Ala-Ala, where superposing on CAs fails\n",
    "idx=tr[0].top.select(\"element != H\")\n",
    "\n",
    "tr.superpose(tr[0],atom_indices=idx)\n",
    "nv.show_mdtraj(tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bd3113-94e0-47b6-b4ec-b075c576dbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gmx pdb2gmx -f alaninedipeptide_H.pdb -o aladip_H.gro -p aladip_H.top -n aladip_H.ndx -water tip3p -ff amber99 -ignh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c483e1-70a1-4968-b526-51795a7554e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(tr.xyz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130f36d9-6466-49eb-b8cb-3d627cc0febb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = .7\n",
    "validation = .15\n",
    "test = .15\n",
    "\n",
    "assert train + validation + test == .9999999999999999 or 1\n",
    "\n",
    "tr_i = len(tr) * train\n",
    "X_train = tr.slice(slice(0,int(tr_i)))\n",
    "\n",
    "va_i = len(tr) * validation\n",
    "X_validate = tr.slice(slice(int(tr_i),int(tr_i)+int(va_i)))\n",
    "\n",
    "te_i = len(tr) * test\n",
    "X_test = tr.slice(slice(int(tr_i)+int(va_i),len(tr)))\n",
    "\n",
    "X_train.xyz.shape, X_validate.xyz.shape, X_test.xyz.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7924d18-7504-48a6-bebe-e8eb420190fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.save_xtc('train.xtc')\n",
    "X_validate.save_xtc('validate.xtc')\n",
    "X_test.save_xtc('test.xtc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03cb7b4-9d71-44aa-a7bc-83fbd6d37744",
   "metadata": {},
   "outputs": [],
   "source": [
    "trajs = [X_train, X_validate, X_test]\n",
    "geoms = []\n",
    "\n",
    "for i in range(len(trajs)):\n",
    "    geoms.append(np.moveaxis(trajs[i].xyz,0,-1))\n",
    "    print(geoms[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c84bb2c-3745-4be3-bb76-ac3b246f8d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.data.Dataset.from_tensor_slices(geoms[0]).save('datasets/geoms/train')\n",
    "tf.data.Dataset.from_tensor_slices(geoms[1]).save('datasets/geoms/validate')\n",
    "tf.data.Dataset.from_tensor_slices(geoms[2]).save('datasets/geoms/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbb67a4-bf24-4b66-b3d2-77a75af35deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mols = []\n",
    "for i in range(len(geoms)):\n",
    "    sparse_dists = asmsa.NBDistancesDense(geoms[i].shape[0])\n",
    "    mols.append(asmsa.Molecule(pdb=conf,top=topol,ndx=index,fms=[sparse_dists]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935d1718-b78e-44e0-a4b1-dd37ca362b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "intcoords = []\n",
    "for i in range(len(mols)):\n",
    "    intcoords.append(mols[i].intcoord(geoms[i]).T)\n",
    "    print(intcoords[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f815dc6-96ab-43fa-9fea-eff18454a1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.data.Dataset.from_tensor_slices(intcoords[0]).save('datasets/intcoords/train')\n",
    "tf.data.Dataset.from_tensor_slices(intcoords[1]).save('datasets/intcoords/validate')\n",
    "tf.data.Dataset.from_tensor_slices(intcoords[2]).save('datasets/intcoords/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32450837-cf5e-4bd9-88eb-c08c389a9c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "[train,validate,test] = intcoords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ff032a-9831-440b-bbda-e9f5defd1c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "\n",
    "hp = _default_hp.copy()\n",
    "hp['learning_rate'] = 0.00005\n",
    "# normal & uniform, 1024\n",
    "train_batch = tf.data.Dataset.from_tensor_slices(train).batch(batch_size,drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb97a28c-608b-495f-a865-b2a3ba506da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _Uniform2d:\n",
    "    def __init__(self,low=-1.,high=1):\n",
    "        self.low = tf.constant(low,tf.float32)\n",
    "        self.high = tf.constant(high,tf.float32)\n",
    "        self.density = 1./(self.high-self.low)**2\n",
    "        self.uniform = tfp.distributions.Uniform(low=low,high=high)\n",
    "\n",
    "    def prob(self,samples):\n",
    "        out = tf.broadcast_to(self.density,shape=[tf.shape(samples)[0]])\n",
    "        out *= tf.cast(samples[:,0] >= self.low,tf.float32)\n",
    "        out *= tf.cast(samples[:,0] <= self.high,tf.float32)\n",
    "        out *= tf.cast(samples[:,1] >= self.low,tf.float32)\n",
    "        out *= tf.cast(samples[:,1] <= self.high,tf.float32)\n",
    "        return out\n",
    "\n",
    "    def sample(self,n):\n",
    "        return self.uniform.sample([n,2])\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c5acb3-fce8-49bb-a0ae-016bbf121a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "\n",
    "class _ImageDist:\n",
    "    def __init__(self,image):\n",
    "        img = np.array(PIL.Image.open(image),dtype=np.float64)\n",
    "        img /= 255.\n",
    "        if len(img.shape) == 3:\n",
    "            img = np.sum(img,axis=2)\n",
    "\n",
    "        assert len(img.shape) == 2\n",
    "        img /= np.sum(img)\n",
    "        \n",
    "        \n",
    "        self.shape = img.shape\n",
    "        self.flat = tf.convert_to_tensor(np.cumsum(img.flatten()).astype(np.float32))\n",
    "        self.img = tf.constant(img * img.shape[0]*img.shape[1], tf.float32)\n",
    "\n",
    "    def sample(self,n):\n",
    "        u = tf.random.uniform(shape=[n])\n",
    "        b = tf.broadcast_to(self.flat,u.shape[:-1]+self.flat.shape)\n",
    "        flati = tf.searchsorted(b,u)\n",
    "        x = tf.cast(flati % self.shape[1],tf.float32)\n",
    "        x += tf.random.uniform(x.shape)\n",
    "        y = tf.cast(flati // self.shape[1],tf.float32)\n",
    "        y += tf.random.uniform(y.shape)\n",
    "        x /= self.shape[1]\n",
    "        y = 1. - y/self.shape[0]\n",
    "        return tf.stack([x,y],axis=-1)\n",
    "\n",
    "    def prob(self,samples):\n",
    "        isamples = tf.cast(tf.stack([1.-samples[:,1],samples[:,0]],axis=1) * self.shape,tf.int32)\n",
    "        return tf.gather_nd(self.img,isamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d782e2-edb6-4c6e-a60f-37ed08984339",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior=tfp.distributions.MultivariateNormalDiag(loc=[0.,0.])\n",
    "#prior = _Uniform2d()\n",
    "#prior = _ImageDist('mushroom_bw.png')\n",
    "mod=BatchedAAEModel(train.shape[1], \n",
    "            ae_layers=[16,8],\n",
    "            disc_layers=[8,8],\n",
    "            prior=prior,\n",
    "            hp=hp)\n",
    "mod.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf17307-5409-44bb-b755-6f146654d85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.fit(train_batch,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b415a5f1-4130-4683-9733-2e65c788e39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_Hist2D(x,min=-1.,max=1.,bins=10):\n",
    "    zeros = tf.zeros((bins,bins),tf.float32)\n",
    "# XXX: bounds\n",
    "    idx = (x - min) * (1./(max - min))\n",
    "#    idx = tf.cast(idx * bins,tf.int32)\n",
    "    return tf.reshape(idx[:50],(10,10))\n",
    "\n",
    "    ones = tf.ones(tf.shape(x)[0])\n",
    "    return tf.tensor_scatter_nd_add(zeros,idx,ones) / tf.cast(tf.shape(x)[0],tf.float32)\n",
    "\n",
    "r = tf.reshape(mod.prior.sample(50),(10,10))\n",
    "r = t_Hist2D(mod.prior.sample(1000),bins=10)\n",
    "with tf.GradientTape() as t:\n",
    "#    lows = tf.reshape(mod.enc(test[:50]),(10,10))\n",
    "    lows = t_Hist2D(mod.enc(test),bins=10)\n",
    "    loss = mod.lossfn(r,lows)\n",
    "\n",
    "grad = t.gradient(loss,mod.enc.trainable_weights)\n",
    "mod.optimizer.apply_gradients(zip(grad,mod.enc.trainable_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fb34db-451a-4331-a747-7fabe6c0b7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "lows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1aaf3a-857a-44ea-9a68-d7f2157a8a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.disc.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071bde88-ea70-45fe-b3be-f6d46d4e04e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lows = mod.enc(test).numpy()\n",
    "\n",
    "plt.scatter(lows[:,0],lows[:,1],marker='.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29835f38-f32e-4fa5-bfb5-262a1b704d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "rg = md.compute_rg(X_test)\n",
    "base = md.load(conf)\n",
    "rmsd = md.rmsd(X_test,base[0])\n",
    "cmap = plt.get_cmap('rainbow')\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(121)\n",
    "plt.scatter(lows[:,0],lows[:,1],marker='.',c=rg,cmap=cmap)\n",
    "plt.colorbar(cmap=cmap)\n",
    "plt.title(\"Rg\")\n",
    "plt.subplot(122)\n",
    "plt.scatter(lows[:,0],lows[:,1],marker='.',c=rmsd,cmap=cmap)\n",
    "plt.colorbar(cmap=cmap)\n",
    "plt.title(\"RMSD\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d712e548-5231-42e5-abb0-e5b620209c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(121)\n",
    "dih=md.compute_dihedrals(X_test,np.array([[4,6,8,14],[6,8,14,16]]))\n",
    "plt.scatter(lows[:,0],lows[:,1],marker='.',c=dih[:,0],cmap=cmap)\n",
    "plt.colorbar(cmap=cmap)\n",
    "plt.subplot(122)\n",
    "plt.scatter(lows[:,0],lows[:,1],marker='.',c=dih[:,1],cmap=cmap)\n",
    "plt.colorbar(cmap=cmap)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3bdc57-e3f0-4fad-a169-b69b2d2beeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_train = mod.enc(train).numpy()\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(121)\n",
    "dih=md.compute_dihedrals(X_train,np.array([[4,6,8,14],[6,8,14,16]]))\n",
    "plt.scatter(low_train[:,0],low_train[:,1],marker='.',c=dih[:,0],cmap=cmap)\n",
    "plt.colorbar(cmap=cmap)\n",
    "plt.subplot(122)\n",
    "plt.scatter(low_train[:,0],low_train[:,1],marker='.',c=dih[:,1],cmap=cmap)\n",
    "plt.colorbar(cmap=cmap)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f19271-0cc9-4de9-99dc-62b5e3b66487",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_test = mod.dec(mod.enc(test))\n",
    "np.mean(k.metrics.mean_squared_error(test,out_test).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa39fcdb-c228-42f8-929f-531c79160bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_train = mod.dec(mod.enc(train))\n",
    "np.mean(k.metrics.mean_squared_error(train,out_train).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766c975e-b0a6-488c-b800-7e837f5d0ecf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37639d10-6c07-4131-96bb-a62bc7fdafee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
